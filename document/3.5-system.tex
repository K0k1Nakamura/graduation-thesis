%!TEX root = ../main.tex
\chapter{IL-RBMの強化学習への応用}
この章は\ref{sec:ilrbm}にて説明したIL-RBMの,本システムにおける強化学習への応用について説明する。


\subsection{強化学習}
強化学習とは、あるエージェントがある環境内にて、得られる報酬を最大化するような行動を学習するような機械学習のことである。

強化学習には重要な4つの概念がある。

\begin{itemize}
  \item 環境 … エージェントの行動に応じて、報酬をエージェントに与える。また、エージェントの行動に応じて、エージェントの観測する環境も更新される。
  \item 報酬 … エージェントの行動に応じて環境からエージェントに与えられる。この得られる報酬を最大化するようエージェントは行動を学習する。
  \item 行動  … エージェントは観測した環境に応じて、行動を選択する。
  \item エージェント … 環境を観測し、報酬を受け取り、行動を選択する。
\end{itemize}

明確な教師データが与えられる教師あり学習や、全く教師データが与えられない教師なし学習と異なり、報酬という限定されたフィードバックのみが与えられる点に特徴がある。
不確実な環境を取り扱えるという点で、応用上非常に有望な機械学習手法の一つである。

\subsection{エージェントの概要}
エージェントの概要について具体的に説明する。

本システムで用いたエージェントは、IL-RBMと出力層からなるDBN(以下、IL-DBN)を用いている。このIL-DBNは入力データを環境、出力を行動として学習する。
エージェントは、常に今自分が置かれている環境を観測することが可能であり、環境から報酬を与えられた場合には、それを感知することが可能である。

エージェントは環境と行動の組からなるデータセットを自動的に構築し、そのデータセットを逐次的に学習することで、最適行動を学習する。
また、IL-RBMのエネルギーに注目することで、未学習データセットを検出できるという特徴を用い、後述するサブゴールを獲得し、長期的な戦略を獲得することが可能である。

\subsection{未学習データ判定法}

\ref{sec:learn}で説明したとおり、RBMは以下に$J$で表される対数尤度を最大化するように学習が行われる。すなわち、エネルギー$E$を最小化するように学習が行われることと同値である。

\begin{eqnarray}
	p(\bm{v}, \bm{h}; \theta) & = & \frac{1}{Z(\theta)}  \exp (-E(\bm{v},\bm{h};\theta)) \nonumber \\
	E(\bm{v}, \bm{h}; \theta) & = & -\sum_i b_i v_i - \sum_j c_j h_j- \sum_i \sum_j v_i W_{ij} h_j \nonumber \\
	p(\bm{v};\theta) & = & \sum_{\bm{h}} p(\bm{v},\bm{h};\theta) \nonumber \\
			& = & \sum_{\bm{h}} \frac{1}{Z(\theta)} \exp (-E(\bm{v}, \bm{h}; \theta)) \nonumber \\
	J & = & < \ln \sum_{\bm{h}} p(\bm{v}, \bm{h}; \theta) >_q \nonumber \\
	  & = & < \ln \sum_{\bm{h}} \exp (-E(\bm{v}, \bm{h}; \theta) >_q - \ln Z(\theta) \nonumber 
\end{eqnarray}

したがって、学習済みRBMは既学習データが入力された場合はエネルギーが低くなり、未学習データが入力された場合にはエネルギーが高くなる。この原理を応用し、あるデータが入力された際のRBMのエネルギーを判定することで未学習データか既学習データかの判定を行う。

未学習データを入力した際のエネルギーと既学習データを入力した際のエネルギーをそれぞれ記録しておき、この値に基づいてある閾値を決定し、未学習か既学習か不明であるデータが入力された際のRBMのエネルギーが、この閾値を上回れば未学習、下回れば既学習と判定する。

\subsection{データセットの獲得}

本システムがデータセットを環境中から自動で学習するメカニズムについて説明する。
以下、三目並べタスクの最適行動を学習するエージェントを例に説明する。

学習エージェントが対戦相手と三目並べを行う環境下で、三目並べに勝利した場合と敗北した場合にそれぞれ正の報酬と負の報酬を与えられるとする。この時、エージェントが観測する環境は盤面の状態であり、出力する行動は次に選択する手の盤面上の位置となる。

\subsubsection{勝利データセットの獲得}
まず、エージェントは勝利データセットを収集する。ある行動を出力した際、盤面が勝利条件を満たし、正の報酬が与えられたとする。その勝利条件を満たした際の行動と、その行動をとった際の環境を組みにしてデータセットとしてエージェントは保存する。

データセットがある一定数に達した場合、または試行回数が一定数に達した場合、エージェントは保存したデータセットを用いて学習を行う。

\subsubsection{サブゴールの獲得}
前述した、既学習、未学習判定を用いてサブゴールを獲得し、長期的な戦略を獲得するメカニズムについて説明する。

勝利データセットの獲得において、盤面が勝利条件を満たした場合、その時にとった行動と、行動を選択した際の環境を組みにしてデータセットとして保存した。
この場合、勝利条件を満たした盤面の状態をゴールとしてデータセットを採集している。

サブゴールとは、ゴールにつながり得る環境の状態を指す。
サブゴールを適切に設定し、サブゴールに至る行動と、その行動を選択した際の環境を更にデータセットに加える事で、長期的な戦略を獲得することができる。

サブゴールの設定方法について説明する。ある行動を選択し、環境が更新されたとする。
その際の環境をエージェントのRBMが既学習であるか未学習であるかを前述したエネルギーによる判定法で判定する。
そして、得られた環境が既学習であった場合、その環境はゴールへと至る可能性が高いため、その環境をサブゴールとして設定する。
そして、サブゴールに至る直前の環境と、その状態にて選択した行動をデータセットとして新たに保存し、追加学習を行う。

このようにして、サブゴールについても学習したIL-RBMは、また新たに"サブゴールのサブゴール"を扱うことが可能になる。観測された環境が、既に学習済みであるサブゴールである環境と一致、近似すると判定された場合、さらにその環境に至る行動とその直前の環境をデータセットとして加えることで、サブゴールのサブゴールを学習でき、長期的な戦略を獲得することが可能である。

\subsection{負のネットワーク、負のサブゴール}
前述したサブゴールの設定法には負の報酬と行動の抑制を扱えない、という欠点があった。
あるRBMがある環境を表す入力データを未学習か既学習か判定できたとしても、その環境が正の報酬に紐付いているか、負の報酬に紐付いているかを判定できないからである。

そこで、負の報酬と負の報酬を得る環境へ至る行動を抑制するために負の報酬をあつかうネットワークをエージェントに追加することにした。

エージェントはIL-RBMを二つ使用する。それぞれ正の報酬を扱うネットワークと負の報酬を扱うネットワークである。
また、保持するデータセットも二種類用意する。正の報酬を得ることのできるゴールに至る行動と、その直前の環境の組、サブゴールに至る行動とその直前の環境の組をデータセットとして扱う正のデータセットと、負の報酬について同様に扱う負のデータセットである。

負のデータセットを扱う負のネットワークでは、負の報酬を得ることになるゴール、サブゴールに至る行動が出力され、その直前の環境が入力データとなる。したがって、観測された環境をこのネットワークに入力した際の出力された行動は抑制されるべき行動である。

本エージェントでは負のネットワークからの出力を正のネットワークからの出力から引くことで、負の報酬を得る環境へ至る行動を抑制している。しかし、正のネットワークと負のネットワークを同列に扱うべきか、荷重をかけどちらかを優先すべきかなどは研究途中である。

