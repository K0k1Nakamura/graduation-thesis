%!TEX root = ../main.tex
\chapter{評価実験}
本研究では、提案手法の強化学習への妥当性の検証のため、三目並べタスクを行った。実験を行うために制作したシステムの詳細について述べた後、実験と結果についての報告と考察を行う。

\section{三目並べタスクの強化学習システムの詳細}
本システムは強化学習の基本的な考え方に則り、環境とエージェントの相互作用を取り扱う。

\subsection{環境部}
環境部はエージェントと環境の相互作用を制御するmainオブジェクトと、三目並べタスクの詳細の動作、情報を制御するThreePieceオブジェクトからなる。

\subsection{mainオブジェクト}
mainオブジェクトは、プログラム全体の制御を行う。

まず、行うタスクを定義する。今回行うタスクは三目並べであるためThreePieceオブジェクトをタスクとして定義する。その後、タスクの要請する数のエージェントを定義する。今回の実験では、先行のエージェントをランダムエージェント、後攻のエージェントをIL-DBNエージェントとした。

その他、エージェントと環境の相互作用を制御し、三目並べの終了時にThreePieceオブジェクトとエージェントのリセットを行ったり、三目並べを何ターン行うかの制御、それぞれのエージェント数の勝利数の保持を行う。

\subsection{ThreePieceオブジェクト}
ThreePieceオブジェクトは三目並べタスクを実際に執り行う。
ThreePieceオブジェクトはターン制でそれぞれのエージェントに盤面の情報を渡し、行動情報を受け取る。
盤面の情報の次元数や行動情報の形式はThreePieceオブジェクトが決定し、エージェントがアーキテクチャをその形式に合わせる。

盤面の情報はそれぞれのマスに対し、空白ならば(0,0),白石が置かれていれば(0,1)、黒石が置かれていれば(1,0)の2bitで表現する。したがって9マス全体の表現は18次元の(0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,1,0)のような表現になる。

一方エージェントの行動は0〜8の整数値で表される。それぞれの数値が石を置く盤面上の位置を示している。

%一方、エージェントの行動は9次元のベクトルで表現される。配置する石の位置に対応する次元の値のみ1で、それ以外は0のベクトルで表す。

ThreePieceオブジェクトはエージェントから渡された石の置き位置を示す値がルール上正当なものかを判定する。エージェントが石を置こうとしている場所に既に石が置かれている場合はエージェントに再度行動を選択するように命令する。指定された石の置き位置が正当であれば、オブジェクトの保持する盤面の状態を更新する。

その後、盤面の状態が三目並べの終了条件を満たしているかを判定する。盤面の状態を監視し、縦、横、斜めいずれかの方向に石が3つ並んでいれば、勝利エージェントへ報酬1を、敗北エージェントへ報酬-1を与える。

\subsection{エージェント部}
エージェントは基本的なエージェントの振る舞いを規定するAgentクラスがあり、それを継承することでそれぞれのAgentのクラスが作られている。

エージェントは環境から盤面状態を受け取った後、行動選択を行う。ランダムエージェントであれば0〜８で乱数を返し、学習エージェントは学習した行動を出力する。

環境が行動を受け取った後、エージェントは環境から報酬を受け取る。学習エージェントは報酬に応じて学習を行う。その学習の具体的なプロセスについては後述する。

\susubbsection{学習エージェント}
学習エージェントは次のような流れで学習を行う。



\begin{enumerate}
  \item データセットを構築する。
  \item
  \item 先行のエージェントは盤面の状態から行動を選択する。
  \item 環境は、エージェントの行動を受け取り、自身の状態を更新する。
  \item 環境の状態に応じて、エージェントに報酬を与える。
  \item 環境の状態が終了条件を満たせばゲームを終了する。
  \item 上記手順を先攻、後攻を交代し繰り返す。
\end{enumerate}


\section{予備実験と各種パラメータの決定}

\section{提案手法の評価実験}

\begin{itemize}
\item [(1)]対話システムの性能として
\item [(2)]ユーモアの質として
\end{itemize}
表\ref{ht:Grice}に(1)の評価項目,\ref{ht:Humor}に(2)の評価項目を示す。