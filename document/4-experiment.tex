%!TEX root = ../main.tex
\chapter{評価実験}
本研究では、提案手法の強化学習への妥当性の検証のため、三目並べタスクを行った。実験を行うために制作したシステムの詳細について述べた後、実験と結果についての報告と考察を行う。

\section{三目並べタスクの強化学習システムの詳細}
本システムは強化学習の基本的な考え方に則り、環境とエージェントの相互作用を取り扱う。

\subsection{環境部}
環境部はエージェントと環境の相互作用を制御するmainオブジェクトと、三目並べタスクの詳細の動作、情報を制御するThreePieceオブジェクトからなる。

\subsubsection{mainオブジェクト}
mainオブジェクトは、プログラム全体の制御を行う。

まず、行うタスクを定義する。今回行うタスクは三目並べであるためThreePieceオブジェクトをタスクとして定義する。その後、タスクの要請する数のエージェントを定義する。今回の実験では、先行のエージェントをランダムエージェント、後攻のエージェントをIL-DBNエージェントとした。

その他、エージェントと環境の相互作用を制御し、三目並べの終了時にThreePieceオブジェクトとエージェントのリセットを行ったり、三目並べを何ターン行うかの制御、それぞれのエージェント数の勝利数の保持を行う。

\subsubsection{ThreePieceオブジェクト}
ThreePieceオブジェクトは三目並べタスクを実際に執り行う。
ThreePieceオブジェクトはターン制でそれぞれのエージェントに盤面の情報を渡し、行動情報を受け取る。
盤面の情報の次元数や行動情報の形式はThreePieceオブジェクトが決定し、エージェントがアーキテクチャをその形式に合わせる。

盤面の情報はそれぞれのマスに対し、空白ならば(0,0),白石が置かれていれば(0,1)、黒石が置かれていれば(1,0)の2bitで表現する。したがって9マス全体の表現は18次元の(0,0,0,1,1,0,0,0,0,1,0,0,0,0,0,0,1,0)のような表現になる。

一方エージェントの行動は0〜8の整数値で表される。それぞれの数値が石を置く盤面上の位置を示している。

%一方、エージェントの行動は9次元のベクトルで表現される。配置する石の位置に対応する次元の値のみ1で、それ以外は0のベクトルで表す。

ThreePieceオブジェクトはエージェントから渡された石の置き位置を示す値がルール上正当なものかを判定する。エージェントが石を置こうとしている場所に既に石が置かれている場合はエージェントに再度行動を選択するように命令する。指定された石の置き位置が正当であれば、オブジェクトの保持する盤面の状態を更新する。

その後、盤面の状態が三目並べの終了条件を満たしているかを判定する。盤面の状態を監視し、縦、横、斜めいずれかの方向に石が3つ並んでいれば、勝利エージェントへ報酬1を、敗北エージェントへ報酬-1を与える。

\subsection{エージェント部}
エージェントは基本的なエージェントの振る舞いを規定するAgentクラスがあり、それを継承することでそれぞれのAgentのクラスが作られている。

エージェントは環境から盤面状態を受け取った後、行動選択を行う。ランダムエージェントであれば0〜８で乱数を返し、学習エージェントは学習した行動を出力する。

環境が行動を受け取った後、エージェントは環境から報酬を受け取る。学習エージェントは報酬に応じて学習を行う。その学習の具体的なプロセスについては後述する。

\subsubsection{学習エージェント}
学習エージェントは次のような流れで学習を行う。


\begin{enumerate}
  \item 環境から状態が与えられる。
  \item 環境が記憶済みの状態の場合、一つ手前の状態とその状態でとった行動をデータセットに追加する。
  \item 行動選択後、環境から報酬が与えられる。
  \item 報酬に応じて状態と行動の組をデータセットに追加する。
  \item 上記の1から4を一定回数繰り返す。
  \item 採集したデータセット用いて追加するRBMのノード数を決定する。
  \item ノード数の確定したRBMを採集したデータセットで学習させる。
  \item データセットを空にリセットし1から再度データセットを採集する。
\end{enumerate}




\section{予備実験と各種パラメータの決定}

学習率、エポック数等の各種パラメータを決定するために予備実験を行った。

\subsection{RBMの学習率とエポック数}
RBMの学習時の学習率は文献を[]を参考に決定した。
学習エージェントが実際に採集した、入力を盤面の状態とし、出力を次に石を置く盤面の位置とするデータセットを利用する。RBMの素子数はほにゃららの自動決定法により決定した。
エポック数は以下のグラフのようになった。
以上の結果より

\subsubsection{出力層の学習率とエポック数}
出力層の学習率とエポック数を決定するため予備実験を行った。文献[]を参考に学習率を決定した。
学習エージェントが実際に採集した、入力を盤面の状態とし、出力を次に石を置く盤面の位置とするデータセットを利用する
RBMの素子数はほにゃららの自動決定法により決定した。
エポック数は以下のグラフのようになった。
以上の結果より


\section{提案手法の評価実験}
以下の条件下で実験を行った。
- 1ターン1000回の三目並べの試行を行う。
- ターンの終了ごとに追加学習を行う。
- 5ターンにわたり勝利数、敗北数、引き分け数を記録する\cite{kiso}

