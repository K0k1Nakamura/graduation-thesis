%!TEX root = ../main.tex
\chapter{強化学習の概要}
機械学習の一分野である強化学習について述べる。

\section{強化学習}

\subsection{強化学習について}

\subsection{代表的な手法}
\subsubsection{モンテカルロ法}
\subsubsection{Q学習}

\section{ニューラルネットワーク}
ニューラルネットワークは、神経科学的な特徴を反映させた計算モデルである。人間の脳のニューロンとシナプスを模しており、ある値を持つノード（人口ニューロン）とその結合荷重によって表現される。

近年、多層に重ねたニューラルネットワークを用いたディープラーニングが画像認識や音声認識などのパターン認識や、データマイニングにおいて非常に目覚ましい成果を上げており、近年注目を集めている。

\section{ニューラルネットワークの問題}
ニューラルネットワークは、多次元量の線形分離不可能なデータを比較的少ない計算量で扱えるといった長所がある一方、以下に列挙するような問題をはらんでいる。

\subsection{破壊的干渉}
あるデータセットを学習済みのニューラルネットワークに対し、新たなデータセットを学習させる際、既に学習したデータを忘却してしまう、という問題がある。これを破壊的干渉という。

異なる性質のデータセットをその違いを考慮し、一つのニューラルネットワークで扱う場合、既学習情報を破壊せず新たなデータセットを学習する必要がある。

このような問題を解決するために、学習データを保持し、新たなデータセットと合わせて学習するという手法が提案された。しかし、このような手法は学習データを保持し続けなければならず、学習時間が長くなり、メモリを大量に必要とする、という欠点がある。

\subsection{メタパラメータの設定}
ニューラルネットワークの学習には、様々なメタパラメータの設定が必要不可欠である。例えば、層の数、それぞれの層におけるノード数、学習率、荷重減衰、モーメンタムなどである。このようなメタパラメータの組み合わせは莫大な数に上り、これらのパラメータの異なるそれぞれのモデルに対し、それぞれ学習を行い予測精度を比較することは、手間と時間が非常にかかる。そのため、これらのメタパラメータの自動決定、最適化は非常に需要のある研究である。

データセットに対し、データセットの特徴を十分学習する素子数の計算する研究がある。

また、学習率の自動決定に関しては、多くの手法が提案されており、その例として、Adagrat、Adamなどが挙げられる。

\subsection{局所解への収束問題}
ニューラルネットワーク、特に多層ニューラルネットワークにおいて、局所解への収束問題は非常に重要な問題である。ニューラルネットワークの学習において基本的に用いられる手法に誤差逆伝播法があるか、この誤差逆伝播法は設定された誤差関数を現象させるようにノード間の結合荷重などのパラメータを調整する。したがって、誤差関数の局所解へ収束していしまい、最適解へ辿りつけないという問題がある。

局所解への収束問題はとりわけ多層ニューラルネットワークにおいて顕著であり、局所解への収束問題を解決するために事前学習(pre-training)という手法が提案されている。


\section{Restricted Boltzmann Machine}
Restricted Boltzmann Machine(RBM)とは、ニューラルネットワークの一種である。

統計的な変動を用いたホップフィールドネットワークの一種であるBoltzmann Machineの、可視層間、隠れ層間の結合を制限したものである。

その学習における結合の重みの変化の過程が、脳における神経科学的な学習則であるヘブ則とも類似しているなど、ニューラルネットワークの現在非常に有力な手法の一つである。

RBMは可視層と隠れ層の二層からなる。可視層に入力データを入れ学習させることで、隠れ層にその特徴をよく表すようなパラメータが出力される、という特徴がある。

通常、可視層、隠れ層の各ノードには0か1の値が入る。可視層に入力データを入れることで、隠れ層の各ノードの取る値の条件付き確率が計算できる。ゆえに、RBMは確率モデルであり生成モデルである。

\section{Deep Belief Network}
Deep Belief Network(DBN)とは、Deep Neural Networkの一種である。

Deep Neural Networkの一般的な学習方法である、誤差逆伝播法の問題の一つに、局所解への収束問題がある。
誤差逆伝播法はある誤差関数を最小化するように各パラメータを変化させる手法であるが、その性質上誤差関数の局所的な解に収束し、誤差関数の最適解へ収束しない現象が起こり得る。
この現象はDeep Neural Networkの層数が増えるに従ってより起こりやすくなる。
この局所解への収束問題を解決する手法として事前学習(pre-training)が考案された。

事前学習の手法にはいくつかあるが、そのうちの一つが前述したRBMを用いるもので、RBMを用いた事前学習を行うDeep Neural NetworkをDBNと呼ぶ。

DBNではRBMを多段に重ねた構造を取る。
そして、一番下の層から入力データをRBMに学習させ、RBMの隠れ層に特徴が現れるよう各パラメータを学習させる。
その後、RBMが生成モデルである利点を活かし、一番下の層に入力データを入れた後に計算される、隠れ層の条件付き確率を次層のRBMにの可視層に入力し、同じように特徴を学習させる。

このようにしてパラメータを学習させたDeep Neural Networkは、それぞれの層において入力データの特徴をうまく抽出するようパラメータが決定されているため、初期パラメータをランダムに決めていた多層パーセプトロンに比べ局所解へ収束しづらい。

こうして事前学習されたネットワークを最後に教師あり学習で学習させる(fine-tuning)手法をDBNと呼ぶ。


