%!TEX root = ../main.tex
\chapter{システムの概要}
この章ではシステムの概要について説明する。


\section{システム全体の構成}
本システムは、強化学習の基本的な考え方に則り、"環境"と"エージェント"が”行動"と"報酬"により相互に影響を及ぼし合うように構成されている。

\subsection{システム全体の流れ}
システムの流れは以下のとおりである。

\begin{enumerate}
  \item 二つのエージェントを定義する。
  \item 先行のエージェントに盤面の状態を与える。
  \item 先行のエージェントは盤面の状態から行動を選択する。
  \item 環境は、エージェントの行動を受け取り、自身の状態を更新する。
  \item 環境の状態に応じて、エージェントに報酬を与える。
  \item 環境の状態が終了条件を満たせばゲームを終了する。
  \item 上記手順を先攻、後攻を交代し繰り返す。
\end{enumerate}



環境部分は、常にある状態をを持ち、その状態はエージェントの行動によって変化させられる。また、環境部はその状態とエージェントの行動によってエージェントに与える報酬を決定する。

\section{Incremental Learning-RBM(IL-RBM)}

この節では、本システムの根幹となるIL-RBMについて説明する。
IL-RBMは追加学習を行うと既学習情報を失うというRBMの欠点を改良したものである。

\subsection{追加学習}
IL-RBMでは追加学習を行う。追加学習とは、既にあるデータセットに対し学習済みのニューラルネットワークが、新たなデータセットに対し、既学習情報を失わない形で学習を行うことである。

通常のRBMの隠れ層のノード数は一定である。しかし、IL-RBMは新たなデータセットを学習する際に隠れ層のノード数を追加する操作を行う。IL-RBMの学習は次のような流れをとる。

\begin{enumerate}
  \item 初期データセットに対しIL-RBMを学習させる。
  \item 追加データセットに対し、適切な追加ノード数を決定し、Il-RBMの隠れ層にノードを追加する。
  \item 上記手順で追加したノードのみを用いて、追加データセットを学習させる。
  \item 上記2、３の手順を追加データセットの分だけ繰り返し行う。
\end{enumerate}



\section{IL-RBMの強化学習への応用}
本システムでは、IL-RBMを用いて強化学習タスクを行う。IL-RBMを用いたエージェントを設定し、このエージェントに、ある環境下において得られる報酬を最大化するような行動を学習させる。

\subsection{強化学習}
強化学習とは、あるエージェントがある環境内にて、得られる報酬を最大化するような行動を学習するような機械学習のことである。

強化学習には重要な4つの概念がある。

\begin{itemize}
  \item 環境 … エージェントの行動に応じて、報酬をエージェントに与える。また、エージェントの行動に応じて、エージェントの観測する環境も更新される。
  \item 報酬 … エージェントの行動に応じて環境からエージェントに与えられる。この得られる報酬を最大化するようエージェントは行動を学習する。
  \item 行動  … エージェントは観測した環境に応じて、行動を選択する。
  \item エージェント … 環境を観測し、報酬を受け取り、行動を選択する。
\end{itemize}

明確な教師データが与えられる教師あり学習や、全く教師データが与えられない教師なし学習と異なり、報酬という限定されたフィードバックのみが与えられる点に特徴がある。
不確実な環境を取り扱えるという点で、応用上非常に有望な機械学習手法の一つである。


\subsection{未学習データセットの判別}
エージェントに与えられたあるデータを既学習であるか、未学習であるかを判別する手法について述べる。
エージェントに与えられたデータの既学習判定にRBMのエネルギーを用いる。RBMは学習済みのデータに対して、エネルギーが低くなる、という特徴を持つため、データを入力した際のRBMのエネルギーを見ることで入力されたデータが既学習であるか、未学習であるかを判別することができる。


\subsection{エージェントの概要}
エージェントの概要について具体的に説明する。

本システムで用いたエージェントは、IL-RBMと出力層からなるDBN(以下、IL-DBN)を用いている。このIL-DBNは入力データを環境、出力を行動として学習する。
エージェントは、常に今自分が置かれている環境を観測することが可能であり、環境から報酬を与えられた場合には、それを感知することが可能である。

エージェントは環境と行動の組からなるデータセットを自動的に構築し、そのデータセットを逐次的に学習することで、最適行動を学習する。
また、IL-RBMのエネルギーに注目することで、未学習データセットを検出できるという特徴を用い、後述するサブゴールを獲得し、長期的な戦略を獲得することが可能である。


\subsection{データセットの獲得}

本システムがデータセットを環境中から自動で学習するメカニズムについて説明する。
以下、三目並べタスクの最適行動を学習するエージェントを例に説明する。

学習エージェントが対戦相手と三目並べを行う環境下で、三目並べに勝利した場合と敗北した場合にそれぞれ正の報酬と負の報酬を与えられるとする。この時、エージェントが観測する環境は盤面の状態であり、出力する行動は次に選択する手の盤面上の位置となる。

\subsubsection{勝利データセットの獲得}
まず、エージェントは勝利データセットを収集する。ある行動を出力した際、盤面が勝利条件を満たし、正の報酬が与えられたとする。その勝利条件を満たした際の行動と、その行動をとった際の環境を組みにしてデータセットとしてエージェントは保存する。

データセットがある一定数に達した場合、または試行回数が一定数に達した場合、エージェントは保存したデータセットを用いて学習を行う。

\subsubsection{サブゴールの獲得}
前述した、既学習、未学習判定を用いてサブゴールを獲得し、長期的な戦略を獲得するメカニズムについて説明する。

勝利データセットの獲得において、盤面が勝利条件を満たした場合、その時にとった行動と、行動を選択した際の環境を組みにしてデータセットとして保存した。
この場合、勝利条件を満たした盤面の状態をゴールとしてデータセットを採集している。

サブゴールとは、ゴールにつながり得る環境の状態を指す。
サブゴールを適切に設定し、サブゴールに至る行動と、その行動を選択した際の環境を更にデータセットに加える事で、長期的な戦略を獲得することができる。

サブゴールの設定方法について説明する。ある行動を選択し、環境が更新されたとする。
その際の環境をエージェントのRBMが既学習であるか未学習であるかを前述したエネルギーによる判定法で判定する。
そして、得られた環境が既学習であった場合、その環境はゴールへと至る可能性が高いため、その環境をサブゴールとして設定する。
そして、サブゴールに至る直前の環境と、その状態にて選択した行動をデータセットとして新たに保存し、追加学習を行う。

このようにして、サブゴールについても学習したIL-RBMは、また新たに"サブゴールのサブゴール"を扱うことが可能になる。観測された環境が、既に学習済みであるサブゴールである環境と一致、近似すると判定された場合、さらにその環境に至る行動とその直前の環境をデータセットとして加えることで、サブゴールのサブゴールを学習でき、長期的な戦略を獲得することが可能である。

\subsection{負のネットワーク、負のサブゴール}
前述したサブゴールの設定法には負の報酬と行動の抑制を扱えない、という欠点があった。
あるRBMがある環境を表す入力データを未学習か既学習か判定できたとしても、その環境が正の報酬に紐付いているか、負の報酬に紐付いているかを判定できないからである。

そこで、負の報酬と負の報酬を得る環境へ至る行動を抑制するために負の報酬をあつかうネットワークをエージェントに追加することにした。

エージェントはIL-RBMを二つ使用する。それぞれ正の報酬を扱うネットワークと負の報酬を扱うネットワークである。
また、保持するデータセットも二種類用意する。正の報酬を得ることのできるゴールに至る行動と、その直前の環境の組、サブゴールに至る行動とその直前の環境の組をデータセットとして扱う正のデータセットと、負の報酬について同様に扱う負のデータセットである。

負のデータセットを扱う負のネットワークでは、負の報酬を得ることになるゴール、サブゴールに至る行動が出力され、その直前の環境が入力データとなる。したがって、観測された環境をこのネットワークに入力した際の出力された行動は抑制されるべき行動である。

本エージェントでは負のネットワークからの出力を正のネットワークからの出力から引くことで、負の報酬を得る環境へ至る行動を抑制している。しかし、正のネットワークと負のネットワークを同列に扱うべきか、荷重をかけどちらかを優先すべきかなどは研究途中である。


