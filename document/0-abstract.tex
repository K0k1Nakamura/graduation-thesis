%!TEX root = ../main.tex
\abstract

本論文ではRBMを改良し，強化学習タスクへの応用を行った．
従来のRBMは学習済みの状態から新たなデータセットを追加学習する場合に，既学習情報を失ってしまうという問題点が存在した．そこで大澤らは追加学習時に素子を追加するという手法を用いたが，このRBMを強化学習タスクへ応用する際に，報酬の正負をネットワーク内で記憶できないという欠点が存在した．
そこで提案された追加学習可能なRBMにさらに二つのネットワークを持たせることで，それぞれのネットワークが正の報酬へ関連付けられるデータセットと，負の報酬へ関連付けられるデータセットを分けて記憶することを可能にした．
また，今回改良したRBMを用いて，強化学習タスクへの応用を行った．評価実験では三目並べを行い，既存手法に比べ，勝率の増加と，負の報酬を取り扱えることに起因する大きな敗北率の低下を示した．
\cite{gregor2015draw}\cite{radford2015unsupervised}\cite{silver2016mastering}