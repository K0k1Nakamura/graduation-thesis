%!TEX root = ../main.tex
\chapter{はじめに}

近年、機械学習についての研究が盛んである。機械学習の学習手法は大きく三つに分類され、それぞれ教師あり学習、教師なし学習、強化学習である。とりわけ、強化学習は複雑で正確な教師データの与えられない実環境において、ロボット制御や最適化をおこなう有望な手法として注目されている。また、近年のディープラーニング (深層学習: Deep Learning) の成功をうけ、ディープニューラルネットワークを用いた強化学習に対する研究も盛んに行われている。そのような研究の中でも最も有名な研究の一つとして、ディープラーニングと強化学習を組み合わせビデオゲームを解いたものがある[Mnih 13]。この研究では、ディープニューラルネットワークを用いて行動価値観数を近似する手法を用い、複数種のビデオゲームにおいて人間を上回る性能を学習させることに成功した。

一方ニューラルネットワークの学習手法に対する研究も盛んに行われている。一般にニューラルネットワークの学習には多くのパラメータを設定する必要がある。また、ニューラルネットワークは、新規のデータを学習させた場合、過去に学習したデータを忘却する、破壊的干渉と呼ばれる現象がおこる [津守 10]。

これらの問題点を解決するため、多くの手法が提案されてきた。ニューラルネットワークの学習におけるパラメータは、学習率、素子数、層数など多く存在する。例えば、学習率の自動決定として、Adagrad[Duchi 11], やAdam[Kingma, 15]といったものが提案されている。

また、素子数についての研究としてBengioらによる研究[]などがある。この研究ではRBMの中間素子数がデータ数＋1であれば理論上データセットを完全に学習できることが示された。しかしながら実際に使用される際のRBMの主要な役割はデータセット内から特徴を抽出することであり、この役割においてデータセットを完全に学習してしまうような素子数を設定することは適切ではない。

ニューラルネットワークは追加学習が出来ないという点について、大澤らは素子を新たに追加するRBMを提案することで、既学習情報を破壊せず追加学習を可能にした[大澤 14]。RBMに対し与えられた学習データを未学習か既学習かを判定し、未学習であれば新たに素子を追加し学習を行うことで、追加学習を行った。

また、大澤は彼らの提案したRBMを強化学習、行動選択タスクに対し応用した。提案したRBMに正の報酬が与えられた際の環境と行動を学習させた。その後、与えられたデータを未学習が既学習かを判定するプロセスを応用することで、新たに選択する行動によってもたらされる環境が、学習済みの環境か否かを判別し、最適行動選択を行った。

しかし、大澤の提案したRBMではデータセットの未学習、既学習は判定できるものの、与えられたデータが正の報酬に関連づいたデータであるのか、負の報酬に関連ついたデータであるのかを判定することが出来ない。

本研究では大澤の提案したRBMを改良し、与えられたデータが正の報酬に関連したデータであるか、府の報酬に関連したデータであるかを判別可能なRBMを提案する。異なるデータセットに対しそれぞれネットワークを割り当てることで、異なる種類のデータに対して未学習、既学習を判定することが可能である。また、正の報酬に関連したデータセットを学習するネットワークと負の報酬に関連したデータセットを学習するネットワークを持つRBMを使用したエージェントを用い、簡単な強化学習タスクへと応用する。正の報酬が与えられた際の環境と行動、負の報酬が与えられた際の環境と行動をエージェントに学習させ、未学習、既学習を判定することで、長期的な観点から正の報酬を獲得し、負の報酬を避けようとする最適行動選択が可能であることを示す。

以下、2章で既存手法の説明をし、3章で提案手法について詳細を述べ、4章で評価実験について示し、5章をまとめとする．

