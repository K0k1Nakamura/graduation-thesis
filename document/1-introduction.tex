%!TEX root = ．．/main．tex
\chapter{序論}

近年，機械学習についての研究が盛んである．機械学習の学習手法は大きく三つに分類され，それぞれ教師あり学習，教師なし学習，強化学習である．

強化学習とは，エージェントがある環境から与えられる報酬を最大化するような最適行動を学習する機械学習の分野である．その一手法であるQ学習では，ある環境における行動の価値を表す行動価値観数を学習し，ある環境における行動価値観数の値が最大値を取るような行動が選択される．

機械学習の三分野のうち，とりわけ強化学習は複雑で正確な教師データの与えられない実環境において，ロボット制御や最適化をおこなう有望な手法として注目されている．また，近年のディープラーニング (深層学習: Deep Learning) の成功をうけ，ディープニューラルネットワークを用いた強化学習に対する研究も盛んに行われている．そのような研究の中でも最も有名な研究の一つとして，ディープラーニングと強化学習を組み合わせビデオゲームを解いたものがある．この研究では，ディープニューラルネットワークを用いて行動価値観数を近似する手法を用い，複数種のビデオゲームにおいて人間を上回る性能を学習させることに成功した．

一方ニューラルネットワークの学習手法に対する研究も盛んに行われている．一般にニューラルネットワークの学習には多くのパラメータを設定する必要がある．また，ニューラルネットワークは，新規のデータを学習させた場合，過去に学習したデータを忘却する，破壊的干渉と呼ばれる現象がおこる \cite{津守研二2010動的環境下で複数タスクを学習するニューラルネットモデル}．

これらの問題点を解決するため，多くの手法が提案されてきた．ニューラルネットワークの学習におけるパラメータは，学習率，素子数，層数など多く存在する．例えば，学習率の自動決定として，Adagrad\cite{duchi2011adaptive}， やAdam\cite{kingma2014adam}といったものが提案されている．

また，素子数についての研究としてBengioらによる研究\cite{le2008representational}などがある．この研究ではRBMの中間素子数がデータ数＋1であれば理論上データセットを完全に学習できることが示された．しかしながら実際に使用される際のRBMの主要な役割はデータセット内から特徴を抽出することであり，この役割においてデータセットを完全に学習してしまうような素子数を設定することは適切ではない．

また，文献\cite{osawa}の中で，RBMのクロスエントロピーに注目した素子数の自動決定法が提案されている．この手法は，本研究で提案するRBMの改良手法のなかでも使用している．

ニューラルネットワークは追加学習が出来ないという点について，大澤らは素子を新たに追加するRBMを提案することで，既学習情報を破壊せず追加学習を行った\cite{osawa}．RBMに対し与えられた学習データを未学習か既学習かを判定し，未学習であれば新たに素子を追加し学習を行うことで，追加学習を行った．

また，その提案されたRBM\cite{osawa}を強化学習，行動選択タスクに対し応用した．提案したRBMに正の報酬が与えられた際の環境と行動を学習させた．その後，与えられたデータを未学習が既学習かを判定するプロセスを応用することで，新たに選択する行動によってもたらされる環境が，学習済みの環境か否かを判別し，最適行動選択を行った．

しかし，文献\cite{osawa}にて提案されたRBMではデータセットの未学習，既学習は判定できるものの，与えられたデータが正の報酬に関連づいたデータであるのか，負の報酬に関連ついたデータであるのかを判定することが出来ない．

本研究では文献\cite{osawa}にて提案されたRBMを改良し，与えられたデータが正の報酬に関連したデータであるか，負の報酬に関連したデータであるかを判別可能なRBMを提案する．異なるデータセットに対しそれぞれネットワークを割り当てることで，異なる種類のデータに対して未学習，既学習を判定することが可能である．また，正の報酬に関連したデータセットを学習するネットワークと負の報酬に関連したデータセットを学習するネットワークを持つRBMを使用したエージェントを用い，簡単な強化学習タスクへと応用する．正の報酬が与えられた際の環境と行動，負の報酬が与えられた際の環境と行動をエージェントに学習させ，未学習，既学習を判定することで，長期的な観点から正の報酬を獲得し，負の報酬を避けようとする最適行動選択が可能であることを示す．



