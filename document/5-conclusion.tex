%!TEX root = ../main.tex
\chapter{結論}
本論文では．IL-RBMを改良し．強化学習タスクへの応用方法を示した．

ニューラルネットワークを用いた強化学習を行うにあたり，ニューラルネットワークにおける有効な追加学習法を考案する必要性があった．

文献\cite{osawa}で提案されたIL-RBMは追加学習が可能であり，与えられたデータが既学習か未学習かを判定することで、自動的に未学習データセットを構築可能であった．またその未学習データセットを用いて追加学習が可能であった．
しかし，IL-RBMはデータの未学習，既学習の判定は可能であるものの，強化学習への応用の際．与えられたデータが正の報酬に関連づくものか．負の報酬に関連づくものかを区別することが出来なかった．

そこで．IL-RBMに正と負の二種類のネットワークを持たせることで．IL-RBMが正の報酬と関連の強いデータと負の報酬と関連の強いデータを区別することが可能であることを示した．

IL-RBMが正のネットワークと負のネットワークを持つことで．既存手法では扱えなかった負のサブゴールを設定可能となり．負の報酬を避けるような長期的な戦略を獲得できることを示した．

また．提案したIL-RBMをもちいたエージェントに三目並べタスクを実際に解かせた．
エージェントは正の報酬へ繋がる行動のデータセットと負の報酬へ繋がる行動のデータセットを採集し．それぞれを提案した正負のネットワークで学習することで．負のネットワークを持たないIL-RBMより高い勝率で勝つことができることを示した．また．負の報酬を避けるような長期的な戦略により．より顕著に敗北率が減少することを示した．

ニューラルネットワークを行う強化学習において、有効な追加学習法を示せたことにより、より抽象度の高い応用例を示すことを今後の課題とする。