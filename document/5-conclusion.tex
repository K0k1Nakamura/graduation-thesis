%!TEX root = ../main.tex
\chapter{結論}
本論文では、IL-RBMを改良し、強化学習タスクへの応用方法を示した。

文献で提案されたIL-RBMは与えられたデータが既学習か未学習かを判定することが可能であるが、強化学習への応用の際、与えられたデータが正の報酬に関連づくものか、負の報酬に関連づくものかを区別することが出来なかった。

そこで、IL-RBMに正と負の二種類のネットワークを持たせることで、IL-RBMが正の報酬と関連の強いデータと負の報酬と関連の強いデータを区別することが可能であることを示した。

IL-RBMが正のネットワークと負のネットワークを持つことで、既存手法では扱えなかった負のサブゴールを設定可能となり、負の報酬を避けるような長期的な戦略を獲得できることを示した。

また、提案したIL-RBMをもちいたエージェントに三目並べタスクを実際に解かせた。
エージェントは正の報酬へ繋がる行動のデータセットと負の報酬へ繋がる行動のデータセットを採集し、それぞれを提案した正負のネットワークで学習することで、負のネットワークを持たないIL-RBMより高い勝率で勝つことができることを示した。また、負の報酬を避けるような長期的な戦略により、より顕著に敗北率が減少することを示した。